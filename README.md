# Awesome CLIP 
This repo collects the research resources based on CLIP (Contrastive Language-Image Pre-Training) proposed by OpenAI. If you would like to contribute, please open an issue.

## CLIP 
- Learning Transferable Visual Models From Natural Language Supervision [[paper](https://arxiv.org/abs/2103.00020)][[code](https://github.com/openai/CLIP)]
- CLIP: Connecting Text and Images [[blog](https://openai.com/blog/clip/)]
- Multimodal Neurons in Artificial Neural Networks [[blog](https://openai.com/blog/multimodal-neurons/)]

## Training
- OpenCLIP (3rd-party, PyTorch) [[code](https://github.com/mlfoundations/open_clip)]  
- Train-CLIP (3rd-party, PyTorch) [[code](https://github.com/Zasder3/train-CLIP)] 
- Paddle-CLIP (3rd-party, PaddlePaddle) [[code](https://github.com/Zasder3/train-CLIP)] 


## Applications

### GAN 
- VQGAN-CLIP [[code](https://github.com/nerdyrodent/VQGAN-CLIP)]
- StyleCLIP: Text-Driven Manipulation of StyleGAN Imagery [[paper](https://arxiv.org/abs/2103.17249)][[code](https://github.com/orpatashnik/StyleCLIP)]
- CLIP Guided Diffusion [[code](https://github.com/afiaka87/clip-guided-diffusion)] 
- CLIP2StyleGAN: Unsupervised Extraction of StyleGAN Edit Directions [[paper](https://arxiv.org/abs/2112.05219)]
- TargetCLIP: Image-Based CLIP-Guided Essence Transfer  [[paper](https://arxiv.org/abs/2110.12427)][[code](https://github.innominds.com/hila-chefer/TargetCLIP)]
- DiffusionCLIP: Text-Guided Diffusion Models for Robust Image Manipulation [[paper](https://arxiv.org/pdf/2110.02711.pdf)][[code](https://github.com/gwang-kim/DiffusionCLIP)]
- clip2latent: Text driven sampling of a pre-trained StyleGAN using denoising diffusion and CLIP [paper](https://arxiv.org/pdf/2210.02347.pdf)][[code](https://github.com/justinpinkney/clip2latent)]

### Object Detection
- Roboflow Zero-shot Object Tracking [[code](https://github.com/roboflow-ai/zero-shot-object-tracking)] 
- Zero-Shot Detection via Vision and Language Knowledge Distillation [[paper](https://arxiv.org/abs/2104.13921)][[code](https://github.com/tensorflow/tpu/tree/master/models/official/detection/projects/vild)]
- Crop-CLIP [[code](https://github.com/vijishmadhavan/Crop-CLIP)]
- Detic: Detecting Twenty-thousand Classes using Image-level Supervision [[paper](https://arxiv.org/abs/2201.02605)][[code](https://github.com/facebookresearch/Detic)] 
- CLIP-TD: CLIP Targeted Distillation for Vision-Language Tasks [[paper](https://arxiv.org/abs/2201.05729)]
- SLIP: Self-supervision meets Language-Image Pre-training [[paper](https://arxiv.org/abs/2112.12750)][[code](https://github.com/facebookresearch/SLIP)]
- ReCLIP: A Strong Zero-Shot Baseline for Referring Expression Comprehension [[paper](https://arxiv.org/pdf/2204.05991.pdf)][[code](https://github.com/allenai/reclip)]  

### Information Retrieval
- Unsplash Image Search [[code](https://github.com/haltakov/natural-language-image-search)]
- CLIP4Clip: An Empirical Study of CLIP for End to End Video Clip Retrieval [[paper](https://arxiv.org/abs/2104.08860)][[code](https://github.com/ArrowLuo/CLIP4Clip)]
- Less is More: ClipBERT for Video-and-Language Learning via Sparse Sampling [[paper](https://arxiv.org/abs/2102.06183)][[code](https://github.com/jayleicn/ClipBERT)]
- Natural Language YouTube Search [[code](https://github.com/haltakov/natural-language-youtube-search)]
- CLIP-as-service: Embed images and sentences into fixed-length vectors with CLIP [[doc](https://github.com/jina-ai/clip-as-service/tree/main/docs)][[code](https://github.com/jina-ai/clip-as-service)]
- clip-retrieval [[code](https://github.com/rom1504/clip-retrieval)]
- A CLIP-Hitchhiker’s Guide to Long Video Retrieval [[paper](https://arxiv.org/pdf/2205.08508.pdf)][code]()]
- CLIP2Video: Mastering Video-Text Retrieval via Image CLIP [[paper](https://arxiv.org/pdf/2106.11097.pdf)][[code](https://github.com/CryhanFang/CLIP2Video)]
- X-CLIP: End-to-End Multi-grained Contrastive Learning for Video-Text Retrieval [[paper](https://arxiv.org/pdf/2207.07285.pdf)][[code](https://github.com/xuguohai/X-CLIP)]

### Representation Learning 
- Wav2CLIP: Learning Robust Audio Representations From CLIP [[code](https://github.com/descriptinc/lyrebird-Wav2CLIP)]
- CLIP-Lite: Information Efficient Visual Representation Learning from Textual Annotation [[paper](https://arxiv.org/abs/2112.07133)]
- RegionCLIP: Region-based Language-Image Pretraining [[paper](https://arxiv.org/pdf/2112.09106.pdf)][[code](https://github.com/microsoft/RegionCLIP)]
- CMA-CLIP: Cross-Modality Attention CLIP for Image-Text Classification [[paper](https://arxiv.org/abs/2112.03562)]
- DenseCLIP: Language-Guided Dense Prediction with Context-Aware Prompting [[paper](https://arxiv.org/pdf/2112.01518.pdf)][[code](https://github.com/raoyongming/DenseCLIP)]
- CyCLIP: Cyclic Contrastive Language-Image Pretraining [[paper](https://arxiv.org/pdf/2205.14459v1.pdf)][[code](https://github.com/goel-shashank/CyCLIP)]
- CLIP-ViP: Adapting Pre-trained Image-Text Model to Video-Language Representation Alignment [[paper](https://arxiv.org/pdf/2209.06430.pdf)][[code](https://github.com/microsoft/XPretrain/tree/main/CLIP-ViP)]
- DetCLIP: Dictionary-Enriched Visual-Concept Paralleled Pre-training for Open-world Detection [[paper](https://arxiv.org/pdf/2209.09407.pdf)][[code](https://github.com/Sense-GVT/DeCLIP)]
- UniCLIP: Unified Framework for Contrastive Language–Image Pre-training [[paper](https://arxiv.org/pdf/2209.13430.pdf)]
- SpeechCLIP: Integrating Speech with Pre-Trained Vision and Language Model [[paper](https://arxiv.org/pdf/2210.00705.pdf)][[code](https://github.com/atosystem/SpeechCLIP)]
- Chinese CLIP: Contrastive Vision-Language Pretraining in Chinese [[paper](https://arxiv.org/pdf/2211.01335.pdf)][[code](https://github.com/OFA-Sys/Chinese-CLIP)]
- PyramidCLIP: Hierarchical Feature Alignment for Vision-language Model Pretraining [[paper](https://arxiv.org/pdf/2204.14095v2.pdf)]
- Learning Visual Representation from Modality-Shared Contrastive Language-Image Pre-training [[paper](https://arxiv.org/pdf/2207.12661.pdf)][[code](https://github.com/Hxyou/MSCLIP)]


### Text-to-3D Generation
- CLIP-Forge: Towards Zero-Shot Text-to-Shape Generation [[paper](https://arxiv.org/pdf/2110.02624.pdf)]
- Text2Mesh: Text-Driven Neural Stylization for Meshes [[paper](https://arxiv.org/abs/2112.03221)][[code](https://github.com/threedle/text2mesh)]
- CLIP-GEN: Language-Free Training of a Text-to-Image Generator with CLIP [[paper](https://arxiv.org/pdf/2203.00386.pdf)]
- CLIPDraw: Exploring Text-to-Drawing Synthesis through Language-Image Encoders [[paper](https://arxiv.org/pdf/2106.14843.pdf)]
- CLIP-NeRF: Text-and-Image Driven Manipulation of Neural Radiance Fields [[paper](https://arxiv.org/pdf/2112.05139.pdf)][[code](https://github.com/cassiePython/CLIPNeRF)]
- MotionCLIP: Exposing Human Motion Generation to CLIP Space [[paper](https://arxiv.org/pdf/2203.08063.pdf)][[code](https://github.com/GuyTevet/MotionCLIP)]
- AvatarCLIP: Zero-Shot Text-Driven Generation and Animation of 3D Avatars [[paper](https://arxiv.org/pdf/2205.08535.pdf)][[code](https://github.com/hongfz16/AvatarCLIP)]

### Text-to-Image Generation
- Big Sleep: A simple command line tool for text to image generation [[code](https://github.com/lucidrains/big-sleep)]
- Deep Daze: A simple command line tool for text to image generation [[code](https://github.com/lucidrains/deep-daze)]
- CLIP-CLOP: CLIP-Guided Collage and Photomontage [[paper](https://arxiv.org/pdf/2205.03146v2.pdf)][[code](https://github.com/deepmind/arnheim)]
- CLIP-GEN: Language-Free Training of a Text-to-Image Generator with CLIP  [[paper](https://arxiv.org/pdf/2203.00386.pdf)][[code](https://github.com/HFAiLab/clip-gen/blob/main/README_en.md)]

### Prompt Learning
- Learning to Prompt for Vision-Language Models [[paper](https://arxiv.org/abs/2109.01134.pdf)][[code](https://github.com/KaiyangZhou/CoOp)]
- Conditional Prompt Learning for Vision-Language Models [[paper](https://arxiv.org/abs/2203.05557.pdf)][[code](https://github.com/KaiyangZhou/CoOp)]
- Prompt-aligned Gradient for Prompt Tuning [[paper](https://arxiv.org/abs/2205.14865.pdf)][[code](https://github.com/BeierZhu/Prompt-align)]
- CLIP-Adapter: Better Vision-Language Models with Feature Adapters [[paper](https://arxiv.org/abs/2110.04544.pdf)][[code](https://github.com/gaopengcuhk/CLIP-Adapter)]
- Learning to Compose Soft Prompts for Compositional Zero-Shot Learning [[paper](https://arxiv.org/abs/2204.03574)] [[code](https://github.com/BatsResearch/csp)]

### Video Understanding
- VideoCLIP: Contrastive Pre-training for Zero-shot Video-Text Understanding [[code](https://github.com/pytorch/fairseq/tree/main/examples/MMPT)]
- FitCLIP: Refining Large-Scale Pretrained Image-Text Models for Zero-Shot Video Understanding Tasks  [[paper](https://arxiv.org/pdf/2203.13371.pdf)][[code](https://github.com/bryant1410/fitclip)]
- Frozen CLIP Models are Efficient Video Learners [[paper](https://arxiv.org/pdf/2208.03550.pdf)][[code](https://github.com/OpenGVLab/efficient-video-recognition)]
- Towards Real-Time Text2Video via CLIP-Guided, Pixel-Level Optimization [[paper](https://arxiv.org/pdf/2210.12826.pdf)][[code](https://github.com/pschaldenbrand/Text2Video)] 
- MovieCLIP: Visual Scene Recognition in Movies [[paper](https://arxiv.org/pdf/2210.11065v2.pdf)]

### Image Captioning
- CLIP prefix captioning [[code](https://github.com/rmokady/CLIP_prefix_caption)]
- CLIPScore: A Reference-free Evaluation Metric for Image Captioning [[paper](https://arxiv.org/abs/2104.08718)]
- ClipCap: CLIP Prefix for Image Captioning [[paper](https://arxiv.org/pdf/2111.09734v1.pdf)][[code](https://github.com/rmokady/CLIP_prefix_caption)]
- Text-Only Training for Image Captioning using Noise-Injected CLIP [[paper](https://arxiv.org/pdf/2211.00575.pdf)][[code](https://github.com/DavidHuji/CapDec)]
- Fine-grained Image Captioning with CLIP Reward [[paper](https://arxiv.org/pdf/2205.13115.pdf)][[code](https://github.com/j-min/CLIP-Caption-Reward)]

### Image Editing 
- HairCLIP: Design Your Hair by Text and Reference Image [[code](https://github.com/wty-ustc/HairCLIP)]
- CLIPstyler: Image Style Transfer with a Single Text Condition [[paper](https://arxiv.org/pdf/2112.00374.pdf)][[code](https://github.com/paper11667/CLIPstyler)]
- CLIPasso: Semantically-Aware Object Sketching [[paper](https://clipasso.github.io/clipasso/static/source/paper_CLIPasso_Semantically_Aware_Object_Sketching.pdf)][[code](https://clipasso.github.io/clipasso/)]
- Image-based CLIP-Guided Essence Transfer [[paper](https://arxiv.org/pdf/2110.12427.pdf)][[code](https://github.com/hila-chefer/TargetCLIP)]
- CLIPDraw: Synthesize drawings to match a text prompt! [[paper](https://arxiv.org/pdf/2106.14843.pdf)][[code](https://github.com/kvfrans/clipdraw)]
- CLIP-CLOP: CLIP-Guided Collage and Photomontage [[paper](https://arxiv.org/pdf/2205.03146.pdf)][[code](https://github.com/deepmind/arnheim)]
- Towards Counterfactual Image Manipulation via CLIP  [[paper](https://arxiv.org/pdf/2207.02812.pdf)][[code](https://github.com/yingchen001/CF-CLIP)]

### Image Segmentation
- CLIMS: Cross Language Image Matching for Weakly Supervised Semantic Segmentation [[paper](https://arxiv.org/pdf/2203.02668.pdf)][[code](https://github.com/CVI-SZU/CLIMS)]
- Image Segmentation Using Text and Image Prompts [[paper](https://arxiv.org/pdf/2112.10003.pdf)][[code](https://github.com/timojl/clipseg)]
- Extract Free Dense Labels from CLIP [[paper](https://arxiv.org/pdf/2112.01071.pdf)][[code](https://github.com/chongzhou96/MaskCLIP)]
- Open-Vocabulary Semantic Segmentation with Mask-adapted CLIP [[paper](https://arxiv.org/pdf/2210.04150.pdf)][[code](https://github.com/facebookresearch/ov-seg)] 

### 3D Recognition
- PointCLIP: Point Cloud Understanding by CLIP [[Paper](https://arxiv.org/pdf/2112.02413.pdf)][[code](https://github.com/zrrskywalker/pointclip)]
- CLIP2Point: Transfer CLIP to Point Cloud Classification with Image-Depth Pre-training [[paper](https://arxiv.org/pdf/2210.01055.pdf)][[code](https://github.com/tyhuang0428/CLIP2Point)]
- MotionCLIP: Exposing Human Motion Generation to CLIP Space [[paper](https://arxiv.org/pdf/2203.08063.pdf)][[code](https://github.com/GuyTevet/MotionCLIP)] 

### Language Tasks
- CLIP Models are Few-shot Learners: Empirical Studies on VQA and Visual Entailment [[paper](https://arxiv.org/pdf/2203.07190v1.pdf)]

### Object Navigation
- CLIP on Wheels: Zero-Shot Object Navigation as Object Localization and Exploration [[paper](https://arxiv.org/pdf/2203.10421.pdf)]

### Audio
- AudioCLIP: Extending CLIP to Image, Text and Audio [[code](https://github.com/AndreyGuzhov/AudioCLIP)]
- Wav2CLIP: Learning Robust Audio Representations from Clip [[paper](https://arxiv.org/pdf/2110.11499.pdf)][[code](https://github.com/descriptinc/lyrebird-wav2clip)] 
- AVE-CLIP: AudioCLIP-based Multi-window Temporal Transformer for Audio Visual Event Localization


### Localization
- Adapting CLIP For Phrase Localization Without Further Training [[paper](https://arxiv.org/pdf/2204.03647.pdf)][[code](https://github.com/pals-ttic/adapting-CLIP)]

### Others
- Multilingual-CLIP [[code](https://github.com/FreddeFrallan/Multilingual-CLIP)]
- CLIP (With Haiku + Jax!) [[code](https://github.com/kingoflolz/CLIP_JAX)]
- CLIP-Event: Connecting Text and Images with Event Structures [[paper](https://arxiv.org/abs/2201.05078)][[code](https://github.com/limanling/clip-event)]
- How Much Can CLIP Benefit Vision-and-Language Tasks? [[paper](https://openreview.net/forum?id=zf_Ll3HZWgy)]
- DeCLIP: Supervision Exists Everywhere: A Data Efficient Contrastive Language-Image Pre-training Paradigm [[paper](https://arxiv.org/abs/2110.05208)][[code](https://github.com/Sense-GVT/DeCLIP)]
- Democratizing Contrastive Language-Image Pre-training: A CLIP Benchmark of Data, Model, and Supervision [[paper](https://arxiv.org/pdf/2203.05796v1.pdf)][[code](https://github.com/sense-gvt/declip)]
- CLIP meets GamePhysics: Towards bug identification in gameplay videos using zero-shot transfer learning [[paper](https://arxiv.org/pdf/2203.11096.pdf)][[code](https://asgaardlab.github.io/CLIPxGamePhysics/)]
- CLIP-Fields: Weakly Supervised Semantic Fields for Robotic Memory [[paper](https://arxiv.org/pdf/2210.05663.pdf)][[code](https://github.com/notmahi/clip-fields)]
- CLIP-Event: Connecting Text and Images with Event Structures [[paper](https://arxiv.org/pdf/2201.05078.pdf)][[code](https://github.com/limanling/clip-event)]

## Acknowledgment
Inspired by [Awesome Visual-Transformer](https://github.com/dk-liang/Awesome-Visual-Transformer).  


