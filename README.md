# Awesome CLIP 
This repo collects the research resources based on CLIP (Contrastive Language-Image Pre-Training) proposed by OpenAI. If you would like to contribute, please open an issue.

## CLIP 
- Learning Transferable Visual Models From Natural Language Supervision [[paper](https://arxiv.org/abs/2103.00020)][[code](https://github.com/openai/CLIP)]
- CLIP: Connecting Text and Images [[blog](https://openai.com/blog/clip/)]
- Multimodal Neurons in Artificial Neural Networks [[blog](https://openai.com/blog/multimodal-neurons/)]

## Training
- OpenCLIP (3rd-party, PyTorch) [[code](https://github.com/mlfoundations/open_clip)]  
- Train-CLIP (3rd-party, PyTorch) [[code](https://github.com/Zasder3/train-CLIP)] 
- Paddle-CLIP (3rd-party, PaddlePaddle) [[code](https://github.com/Zasder3/train-CLIP)] 


## Applications

### GAN 
- VQGAN-CLIP [[code](https://github.com/nerdyrodent/VQGAN-CLIP)]
- StyleCLIP: Text-Driven Manipulation of StyleGAN Imagery [[paper](https://arxiv.org/abs/2103.17249)][[code](https://github.com/orpatashnik/StyleCLIP)]
- CLIP Guided Diffusion [[code](https://github.com/afiaka87/clip-guided-diffusion)] 
- CLIP2StyleGAN: Unsupervised Extraction of StyleGAN Edit Directions [[paper](https://arxiv.org/abs/2112.05219)]
- TargetCLIP: Image-Based CLIP-Guided Essence Transfer  [[paper](https://arxiv.org/abs/2110.12427)][[code](https://github.innominds.com/hila-chefer/TargetCLIP)]
- DiffusionCLIP: Text-Guided Diffusion Models for Robust Image Manipulation [[paper](https://arxiv.org/pdf/2110.02711.pdf)][[code](https://github.com/gwang-kim/DiffusionCLIP)]

### Object Detection
- Roboflow Zero-shot Object Tracking [[code](https://github.com/roboflow-ai/zero-shot-object-tracking)] 
- Zero-Shot Detection via Vision and Language Knowledge Distillation [[paper](https://arxiv.org/abs/2104.13921)][[code](https://github.com/tensorflow/tpu/tree/master/models/official/detection/projects/vild)]
- Crop-CLIP [[code](https://github.com/vijishmadhavan/Crop-CLIP)]
- Detic: Detecting Twenty-thousand Classes using Image-level Supervision [[paper](https://arxiv.org/abs/2201.02605)][[code](https://github.com/facebookresearch/Detic)] 
- CLIP-TD: CLIP Targeted Distillation for Vision-Language Tasks [[paper](https://arxiv.org/abs/2201.05729)]
- SLIP: Self-supervision meets Language-Image Pre-training [[paper](https://arxiv.org/abs/2112.12750)][[code](https://github.com/facebookresearch/SLIP)]
- ReCLIP: A Strong Zero-Shot Baseline for Referring Expression Comprehension [[paper](https://arxiv.org/pdf/2204.05991.pdf)][[code](https://github.com/allenai/reclip)]  

### Information Retrieval
- Unsplash Image Search [[code](https://github.com/haltakov/natural-language-image-search)]
- CLIP4Clip: An Empirical Study of CLIP for End to End Video Clip Retrieval [[paper](https://arxiv.org/abs/2104.08860)][[code](https://github.com/ArrowLuo/CLIP4Clip)]
- Less is More: ClipBERT for Video-and-Language Learning via Sparse Sampling [[paper](https://arxiv.org/abs/2102.06183)][[code](https://github.com/jayleicn/ClipBERT)]
- Natural Language YouTube Search [[code](https://github.com/haltakov/natural-language-youtube-search)]
- CLIP-as-service: Embed images and sentences into fixed-length vectors with CLIP [[doc](https://github.com/jina-ai/clip-as-service/tree/main/docs)][[code](https://github.com/jina-ai/clip-as-service)]
- clip-retrieval [[code](https://github.com/rom1504/clip-retrieval)]
- A CLIP-Hitchhikerâ€™s Guide to Long Video Retrieval [[paper](https://arxiv.org/pdf/2205.08508.pdf)][code]()]
- CLIP2Video: Mastering Video-Text Retrieval via Image CLIP [[paper](https://arxiv.org/pdf/2106.11097.pdf)][[code](https://github.com/CryhanFang/CLIP2Video)]

### Representation Learning
- Wav2CLIP: Learning Robust Audio Representations From CLIP [[code](https://github.com/descriptinc/lyrebird-Wav2CLIP)]
- CLIP-Lite: Information Efficient Visual Representation Learning from Textual Annotation [[paper](https://arxiv.org/abs/2112.07133)]
- RegionCLIP: Region-based Language-Image Pretraining [[paper](https://arxiv.org/pdf/2112.09106.pdf)][[code](https://github.com/microsoft/RegionCLIP)]
- CMA-CLIP: Cross-Modality Attention CLIP for Image-Text Classification [[paper](https://arxiv.org/abs/2112.03562)]
- DenseCLIP: Language-Guided Dense Prediction with Context-Aware Prompting [[paper](https://arxiv.org/pdf/2112.01518.pdf)][[code](https://github.com/raoyongming/DenseCLIP)]
- CyCLIP: Cyclic Contrastive Language-Image Pretraining [[paper](https://arxiv.org/pdf/2205.14459v1.pdf)][[code](https://github.com/goel-shashank/CyCLIP)]

### Text-to-3D Generation
- CLIP-Forge: Towards Zero-Shot Text-to-Shape Generation [[paper](https://arxiv.org/pdf/2110.02624.pdf)] [[code](https://github.com/autodeskailab/clip-forge)]
- Text2Mesh: Text-Driven Neural Stylization for Meshes [[paper](https://arxiv.org/abs/2112.03221)][[code](https://github.com/threedle/text2mesh)]
- CLIP-GEN: Language-Free Training of a Text-to-Image Generator with CLIP [[paper](https://arxiv.org/pdf/2203.00386.pdf)] [[3rd-party code](https://github.com/HFAiLab/clip-gen)]
- CLIPDraw: Exploring Text-to-Drawing Synthesis through Language-Image Encoders [[paper](https://arxiv.org/pdf/2106.14843.pdf)]
- CLIP-NeRF: Text-and-Image Driven Manipulation of Neural Radiance Fields [[paper](https://arxiv.org/pdf/2112.05139.pdf)][[code](https://github.com/cassiePython/CLIPNeRF)]
- MotionCLIP: Exposing Human Motion Generation to CLIP Space [[paper](https://arxiv.org/pdf/2203.08063.pdf)][[code](https://github.com/GuyTevet/MotionCLIP)]
- AvatarCLIP: Zero-Shot Text-Driven Generation and Animation of 3D Avatars [[paper](https://arxiv.org/pdf/2205.08535.pdf)][[code](https://github.com/hongfz16/AvatarCLIP)]

### Text-to-Image Generation
- Big Sleep: A simple command line tool for text to image generation [[code](https://github.com/lucidrains/big-sleep)]
- Deep Daze: A simple command line tool for text to image generation [[code](https://github.com/lucidrains/deep-daze)]
- CLIP-CLOP: CLIP-Guided Collage and Photomontage [[paper](https://arxiv.org/pdf/2205.03146v2.pdf)][[code](https://github.com/deepmind/arnheim)]

### Prompt Learning
- Learning to Prompt for Vision-Language Models [[paper](https://arxiv.org/abs/2109.01134.pdf)][[code](https://github.com/KaiyangZhou/CoOp)]
- Conditional Prompt Learning for Vision-Language Models [[paper](https://arxiv.org/abs/2203.05557.pdf)][[code](https://github.com/KaiyangZhou/CoOp)]
- Prompt-aligned Gradient for Prompt Tuning [[paper](https://arxiv.org/abs/2205.14865.pdf)][[code](https://github.com/BeierZhu/Prompt-align)]
- CLIP-Adapter: Better Vision-Language Models with Feature Adapters [[paper](https://arxiv.org/abs/2110.04544.pdf)][[code](https://github.com/gaopengcuhk/CLIP-Adapter)]

### Video Understanding
- VideoCLIP: Contrastive Pre-training for Zero-shot Video-Text Understanding [[code](https://github.com/pytorch/fairseq/tree/main/examples/MMPT)]
- FitCLIP: Refining Large-Scale Pretrained Image-Text Models for Zero-Shot Video Understanding Tasks  [[paper](https://arxiv.org/pdf/2203.13371.pdf)][[code](https://github.com/bryant1410/fitclip)]

### Image Captioning
- CLIP prefix captioning [[code](https://github.com/rmokady/CLIP_prefix_caption)]
- CLIPScore: A Reference-free Evaluation Metric for Image Captioning [[paper](https://arxiv.org/abs/2104.08718)] [[code](https://github.com/jmhessel/clipscore)]
- ClipCap: CLIP Prefix for Image Captioning [[paper](https://arxiv.org/pdf/2111.09734v1.pdf)][[code](https://github.com/rmokady/CLIP_prefix_caption)]

### Image Editing 
- HairCLIP: Design Your Hair by Text and Reference Image [[code](https://github.com/wty-ustc/HairCLIP)]
- CLIPstyler: Image Style Transfer with a Single Text Condition [[paper](https://arxiv.org/pdf/2112.00374.pdf)][[code](https://github.com/paper11667/CLIPstyler)]
- CLIPasso: Semantically-Aware Object Sketching [[paper](https://clipasso.github.io/clipasso/static/source/paper_CLIPasso_Semantically_Aware_Object_Sketching.pdf)][[code](https://clipasso.github.io/clipasso/)]

### 3D Recognition
- PointCLIP: Point Cloud Understanding by CLIP [[Paper](https://arxiv.org/pdf/2112.02413.pdf)][[code](https://github.com/zrrskywalker/pointclip)]

### Language Tasks
- CLIP Models are Few-shot Learners: Empirical Studies on VQA and Visual Entailment [[paper](https://arxiv.org/pdf/2203.07190v1.pdf)]

### Object Navigation
- CLIP on Wheels: Zero-Shot Object Navigation as Object Localization and Exploration [[paper](https://arxiv.org/pdf/2203.10421.pdf)]

### Audio
- AudioCLIP: Extending CLIP to Image, Text and Audio [[code](https://github.com/AndreyGuzhov/AudioCLIP)]

### Localization
- Adapting CLIP For Phrase Localization Without Further Training [[paper](https://arxiv.org/pdf/2204.03647.pdf)][[code](https://github.com/pals-ttic/adapting-CLIP)]

### Segmentation
- CLIMS: Cross Language Image Matching for Weakly Supervised Semantic Segmentation [[paper](https://arxiv.org/pdf/2203.02668.pdf)][[code](https://github.com/CVI-SZU/CLIMS)]

### Others
- Multilingual-CLIP [[code](https://github.com/FreddeFrallan/Multilingual-CLIP)]
- CLIP (With Haiku + Jax!) [[code](https://github.com/kingoflolz/CLIP_JAX)]
- CLIP-Event: Connecting Text and Images with Event Structures [[paper](https://arxiv.org/abs/2201.05078)][[code](https://github.com/limanling/clip-event)]
- How Much Can CLIP Benefit Vision-and-Language Tasks? [[paper](https://openreview.net/forum?id=zf_Ll3HZWgy)] [[code](https://github.com/clip-vil/CLIP-ViL)]
- DeCLIP: Supervision Exists Everywhere: A Data Efficient Contrastive Language-Image Pre-training Paradigm [[paper](https://arxiv.org/abs/2110.05208)][[code](https://github.com/Sense-GVT/DeCLIP)]
- Democratizing Contrastive Language-Image Pre-training: A CLIP Benchmark of Data, Model, and Supervision [[paper](https://arxiv.org/pdf/2203.05796v1.pdf)][[code](https://github.com/sense-gvt/declip)]
- CLIP meets GamePhysics: Towards bug identification in gameplay videos using zero-shot transfer learning [[paper](https://arxiv.org/pdf/2203.11096.pdf)][[code](https://asgaardlab.github.io/CLIPxGamePhysics/)]

## Acknowledgment
Inspired by [Awesome Visual-Transformer](https://github.com/dk-liang/Awesome-Visual-Transformer).  


